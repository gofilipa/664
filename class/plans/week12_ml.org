* machine learning / text generation
** introduction to HuggingFace website
- hf platform with "hubs" - like github but more
- "models hub" - models uploaded by users & companies
  - trained & fine-tuned (what's the difference?)
  - look at the name to see if: base, size, version, type.
  - filtering: by task; sorting: by most downloaded
  - stable diffusion - pretty cool
  - gpt-2 - not great
- gpt-neo-125m
  - the license is MIT, permissable, trained on the Pile
  - can also see datasets
  - "use in Transformers"

** inference
If on Colab
#+begin_src
%capture
%pip install transformers trl
#+end_src

If not, will need to install via command line

#+begin_src python
from transformers import pipeline

pipe = pipeline("text-generation", model="EleutherAI/gpt-neo-125m")
#+end_src

Here we have a function, called `pipeline()`, which takes parameters
that specify the task and the model that we will be using. We save the
function to a variable called `pipe`, which we will later use to
process our prompt.

#+begin_src python
  prompt = "Hello, my name is Filipa and"

  output = pipe(prompt, max_length = 50)

  type(output)
#+end_src

We have a dict within a list. How do we grab just the text?

Import datasets, and check out the dataset of "bedtime stories

#+begin_src python
  from datasets import load_dataset
  # load the dataset and its subset
  dataset = load_dataset("gofilipa/bedtime_stories")

  # check the dataset object
  dataset['train']['stories'][0]
#+end_src

Challenge: write a loop to pass a list of phrases to pass into the
prompt.

#+begin_src python
    outputs = []
    for i in dataset['train']['stories'][:5]:
      out = pipe(i, max_new_tokens=100)
      outputs.append(out)
#+end_src

** fine-tune
Loading up our libraries, model, and data
#+begin_src python
    from datasets import load_dataset
    from transformers import (
	AutoModelForCausalLM,
	AutoTokenizer,
	TrainingArguments,
	pipeline
    )
    from trl import SFTTrainer

    tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neo-125m")
  model = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-neo-125m")
  dataset = load_dataset("gofilipa/bedtime_stories")

# Padding is necessary to account for different sizes of text in our dataset.  
  tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"
#+end_src

Training parameters

#+begin_src python
    training_params = TrainingArguments(
      output_dir="./results",
      num_train_epochs = 3, # how many times we iterate over the dataset as a whole
      learning_rate = 2e-4, # how many "steps" we take in adjusting the parameters to make up for loss
      weight_decay = 0.001, # way of regularizing the parameters
      report_to="none" # this is a new param, to avoid a login to W&B
  )
    trainer = SFTTrainer(
    model = model,
    train_dataset = dataset['train'],
    dataset_text_field = "stories",
    tokenizer = tokenizer,
    args = training_params
)
#+end_src

Finally, we train.

~trainer.train()~

If we want to save the new model.

#+begin_src python
    model = AutoModelForCausalLM.from_pretrained("./models")
  tokenizer = AutoTokenizer.from_pretrained("./models")

#+end_src

To run inference

#+begin_src python
    pipe = pipeline('text-generation', model=model,
  tokenizer=tokenizer, max_length=50)

    pipe("There once was a little girl named Filipa and")

#+end_src

