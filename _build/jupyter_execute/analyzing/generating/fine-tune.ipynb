{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c191ca2c",
   "metadata": {},
   "source": [
    "# fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10d47361",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for google colab:\n",
    "# %%capture\n",
    "# %pip install transformers trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf058b05",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     AutoModelForCausalLM,\n\u001b[1;32m      4\u001b[0m     AutoTokenizer,\n\u001b[1;32m      5\u001b[0m     TrainingArguments,\n\u001b[1;32m      6\u001b[0m     pipeline\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer, SFTConfig\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "507fd22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d49a1c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"gofilipa/bedtime_stories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16523909",
   "metadata": {},
   "source": [
    "Padding is necessary to account for different sizes of text in our dataset. \n",
    "\n",
    "*From the [docs on ðŸ¤—](https://huggingface.co/docs/transformers/en/pad_truncation): \n",
    "Batched inputs are often different lengths, so they canâ€™t be converted to fixed-size tensors. Padding and truncation are strategies for dealing with this problem, to create rectangular tensors from batches of varying lengths. Padding adds a special padding token to ensure shorter sequences will have the same length as either the longest sequence in a batch or the maximum length accepted by the model. Truncation works in the other direction by truncating long sequences. In most cases, padding your batch to the length of the longest sequence and truncating to the maximum length a model can accept works pretty well.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bdbeb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9fa6fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs = 3, # how many times we iterate over the dataset as a whole\n",
    "    learning_rate = 2e-4, # how many \"steps\" we take in adjusting the parameters to make up for loss\n",
    "    weight_decay = 0.001, # way of regularizing the parameters\n",
    "    dataset_text_field = \"stories\",\n",
    "    report_to=\"none\" # this is a new param, to avoid a login to W&B\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb85b88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset['train'],\n",
    "    processing_class = tokenizer,\n",
    "    args = training_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69d7dfb8-6a94-4c79-a720-bfb63ceb36fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=1.781717529296875, metrics={'train_runtime': 54.1603, 'train_samples_per_second': 11.023, 'train_steps_per_second': 1.385, 'total_flos': 40251401496576.0, 'train_loss': 1.781717529296875})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### commenting out this line so it doesn't run when I create this website\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ec264",
   "metadata": {},
   "source": [
    "<!--this cell is in markdown to avoid actually training a model when building the book-->\n",
    "```python\n",
    "trainer.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc7f50",
   "metadata": {},
   "source": [
    "What's happening in the training process? Basically the process includes three functions:\n",
    "- hypothesis function that makes a guess as to what word to generate following a given word\n",
    "- loss function that calculates the difference between the guess and the actual word\n",
    "- gradient descent that very slowly updates numbers as to minimize the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2142e7-252e-4248-9fcd-43bfd1d80bae",
   "metadata": {},
   "source": [
    "## running inference on our new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0309a040-11bd-4c54-9707-ed709d6de0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'There once was a little girl named Filipa and her momma. Every night, she would go outside and look up at the stars in the sky. One night, she saw a magical rainbow that would bring her all the joy she wanted. She'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# commenting out this code so that it doesn't run (when I create this website)!\n",
    "\n",
    "### first, save the model to a folder called \"models\" \n",
    "# trainer.model.save_pretrained(\"models\")\n",
    "# trainer.tokenizer.save_pretrained(\"models\")\n",
    "\n",
    "### then, load our model from that folder\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"./models\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"./models\")\n",
    "\n",
    "### create a pipe() function that calls our new model\n",
    "# pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=50)\n",
    "\n",
    "### run inference!\n",
    "pipe(\"There once was a little girl named Filipa and\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}